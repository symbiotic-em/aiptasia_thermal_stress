
#on hpc linux 

#run fastqc for quality control
#make dir for results
mkdir /project/user_name/folderofyourchoice
#run fastqc
fastqc *.fastq.gz

# move the results to your folder
 mv *fastqc* /project/user_name/folderofyourchoice

#unzip, using loop from https://datacarpentry.org/2015-09-21-Genentech/lessons/06-readQC.html
for zip in *.zip; do echo File $zip; unzip $zip; done

#save the .txt files
cat */summary.txt > all_seq_summaries.txt

#remove bad, low read samples as indicated byfastq
Sym Heat1-1, 2-4,3-6; Sym ctrl 1-1; Apo ctrl 1-2,1-6,2-6
#7 samples total out of 59, so 52 left

salloc --time=8:00:00 --cpus-per-task=16 --mem=16GB 
#These sequences should've been demultiplexed already by the sequencing facility
#and they should be free of any primers. Our sequencing company 
#demultiplexed for us and we started off with fastq files.
#Please see https://benjjneb.github.io/dada2/tutorial.html for an in-depth
#tutorial and more info. This script is a very close adaptation of that tutorial.  

R

library(dada2); packageVersion("dada2"); citation("dada2")

#set environment
path <- "/Users/emily/mypath"
fns<-list.files(path)
fns

#forward and reverse fastq filenames
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

#sample names
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
#inspect quality read profiles
pdf(file="ts_anemoneqp1.pdf")
plotQualityProfile(fnFs[1:10])
dev.off()

pdf(file="qp2_ts_anemone.pdf")
plotQualityProfile(fnRs[1:10])
dev.off()

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

#truncLen can be modified
#I used this because of our primers and Miseq 2x250bp that we used to sequence 

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=FALSE) # On Windows set multithread=FALSE, if it's set to TRUE it might not run 

head(out)
tail(out)

#use this tool if you dont know what to use for truncLen for your particular primer, check out this tool:https://github.com/Zymo-Research/figaro#figaro

#error rates
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

pdf(file="final_error_rates_forward.pdf")
plotErrors(errF, nominalQ=TRUE)
dev.off()

pdf(file="final_error_rates_reverse.pdf")
plotErrors(errR, nominalQ=TRUE)
dev.off()



#apply core sample inference algorithm to dereplicated data
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
dadaFs[[1]]
dadaRs[[1]]

#merged paired data
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
summary((mergers[[1]]))

#make your seq data
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

pdf(file="plot_table_seqs_all.pdf")
plot(table(nchar(getSequences(seqtab))))
dev.off()


#### Optional
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(286,301)] #again, being fairly conservative wrt length but this is where most sequences were located according to the plot
table(nchar(getSequences(seqtab2)))
dim(seqtab2)

#NOW MOVE ON with seqtab 2 but can always go back and modify for seqtab
remove chimeras
#The core dada method removes substitution and indel errors, but chimeras remain. 
#Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
#than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
#a bimera (two-parent chimera) from more abundant sequences.

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim) #52, 4577

sum(seqtab.nochim)/sum(seqtab2) #0.9556048

#The fraction of chimeras varies based on factors including experimental procedures and sample complexity, 

#track reads through pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
write.csv(track,file="anemone_filter_stats.csv",row.names=TRUE, quote=FALSE)

#assign taxonomy
#download the SILVA training set of your choice. I used v138. Place this in your path (folder). 
 
taxa <- assignTaxonomy(seqtab.nochim, "silva_nr99_v138_train_set.fa.gz", multithread=TRUE)


#inspect taxa
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
tail(taxa.print)

#saveRDS object to work out in R on laptop
#saveRDS(seqtab1.nochim, file="seqtab_anemone.rds")

saveRDS(seqtab.nochim, file="seqtab_anemone.rds")
saveRDS(taxa, file="taxa_anemone.rds")

#make sure you can read your rds object
tax=readRDS("taxa_anemone.rds")
seq=readRDS("seqtab_anemone.rds")
#and alternately, write your csv 
write.csv(tax, "taxa_anemone.csv")
